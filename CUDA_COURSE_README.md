# FlashInfer CUDA ç¼–ç¨‹è¯¾ç¨‹ - æ–‡ä»¶è¯´æ˜

## ğŸ“š å·²åˆ›å»ºçš„è¯¾ç¨‹æ–‡ä»¶

### ä¸»ç›®å½•
- **cuda_course_index.md** (4.2KB) - è¯¾ç¨‹æ€»è§ˆä¸å­¦ä¹ è·¯çº¿å›¾

### è¯¦ç»†è¯¾ç¨‹ï¼ˆç¬¬ 1-3 å¤©ï¼‰

#### âœ… ç¬¬ 1 å¤©ï¼šCUDA ç¼–ç¨‹åŸºç¡€ä¸ FlashInfer ç¯å¢ƒæ­å»º (11KB)
**æ–‡ä»¶**: `cuda_course_day01.md`

**å†…å®¹**:
- CUDA ç¼–ç¨‹æ¨¡å‹ï¼ˆGrid, Block, Thread, Warpï¼‰
- FlashInfer ä»£ç ç»“æ„ä¸è®¾è®¡ç†å¿µ
- JIT ç¼–è¯‘å·¥ä½œåŸç†
- ç¯å¢ƒæ­å»ºæ­¥éª¤
- ç¬¬ä¸€ä¸ª FlashInfer ç¨‹åº

**å­¦ä¹ æ–‡ä»¶**: `include/flashinfer/vec_dtypes.cuh`

---

#### âœ… ç¬¬ 2 å¤©ï¼šå†…å­˜åˆå¹¶ä¸å‘é‡åŒ–è®¿é—® (13KB)
**æ–‡ä»¶**: `cuda_course_day02.md`

**å†…å®¹**:
- GPU å†…å­˜è®¿é—®æ¨¡å¼
- å†…å­˜åˆå¹¶ï¼ˆCoalescingï¼‰åŸç†ä¸å®è·µ
- å‘é‡åŒ–æŠ€æœ¯ï¼ˆ16å­—èŠ‚åŠ è½½ï¼‰
- `vec_t<T, N>` æ¨¡æ¿è¯¦è§£
- Grid-Stride Loop æ¨¡å¼
- `__restrict__` å…³é”®å­—

**å­¦ä¹ æ–‡ä»¶**: `include/flashinfer/activation.cuh`

**å®éªŒ**:
- æ€§èƒ½æµ‹è¯•å¯¹æ¯”
- å¸¦å®½åˆ©ç”¨ç‡è®¡ç®—

---

#### âœ… ç¬¬ 3 å¤©ï¼šPTX æ±‡ç¼–ä¸åº•å±‚æŒ‡ä»¤ (13KB)
**æ–‡ä»¶**: `cuda_course_day03.md`

**å†…å®¹**:
- PTX æ±‡ç¼–ç®€ä»‹
- å†…è”æ±‡ç¼–è¯­æ³•ï¼ˆ`asm volatile`ï¼‰
- æ•°å­¦æŒ‡ä»¤ï¼šexp2, log2, rsqrt, tanh
- Warp shuffle æŒ‡ä»¤è¯¦è§£
- Butterfly shuffle ç®—æ³•
- ç±»å‹åŒå…³ï¼ˆType Punningï¼‰
- Warp reduction å®ç°

**å­¦ä¹ æ–‡ä»¶**: `include/flashinfer/math.cuh`

**å®éªŒ**:
- PTX æŒ‡ä»¤ vs æ ‡å‡†åº“æ€§èƒ½å¯¹æ¯”
- Warp shuffle reduction

---

### è¯¾ç¨‹å¤§çº²ï¼ˆç¬¬ 4-14 å¤©ï¼‰

#### ğŸ“ ç¬¬ 4-14 å¤©è¯¾ç¨‹å¤§çº²
**æ–‡ä»¶**: `cuda_course_day04_to_14.md` (4.0KB)

åŒ…å«ç¬¬ 4-14 å¤©çš„å®Œæ•´å­¦ä¹ è·¯çº¿ï¼š

**ç¬¬ 4 å¤©**: å…±äº«å†…å­˜åŸºç¡€
**ç¬¬ 5 å¤©**: Warp çº§ç¼–ç¨‹
**ç¬¬ 6 å¤©**: å¤šçº§ Reduction
**ç¬¬ 7 å¤©**: å¼‚æ­¥å†…å­˜æ‹·è´
**ç¬¬ 8 å¤©**: CUB åº“ä¸å¤æ‚æ•°æ®ç»“æ„
**ç¬¬ 9 å¤©**: Tensor Core ç¼–ç¨‹åŸºç¡€
**ç¬¬ 10 å¤©**: çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
**ç¬¬ 11 å¤©**: Attention å†…æ ¸ï¼ˆDecodeï¼‰
**ç¬¬ 12 å¤©**: Attention å†…æ ¸ï¼ˆPrefillï¼‰
**ç¬¬ 13 å¤©**: JIT ç¼–è¯‘ç³»ç»Ÿ
**ç¬¬ 14 å¤©**: æ¡†æ¶é›†æˆä¸å®Œæ•´æµç¨‹

æ¯å¤©åŒ…å«ï¼š
- å­¦ä¹ ç›®æ ‡
- æŒ‡å®šæºæ–‡ä»¶
- æ ¸å¿ƒæ¦‚å¿µ
- ä»£ç ç¤ºä¾‹
- å®è·µä½œä¸š

---

## ğŸš€ å¦‚ä½•å¼€å§‹å­¦ä¹ 

### 1. æŸ¥çœ‹è¯¾ç¨‹æ€»è§ˆ
```bash
cat cuda_course_index.md
```

### 2. æŒ‰é¡ºåºå­¦ä¹ æ¯ä¸€å¤©
```bash
# ç¬¬1å¤©
cat cuda_course_day01.md

# ç¬¬2å¤©
cat cuda_course_day02.md

# ç¬¬3å¤©
cat cuda_course_day03.md

# ç¬¬4-14å¤©å¤§çº²
cat cuda_course_day04_to_14.md
```

### 3. é˜…è¯»å¯¹åº”çš„æºä»£ç 
æ¯å¤©çš„è¯¾ç¨‹éƒ½ä¼šæŒ‡å®šè¦å­¦ä¹ çš„ FlashInfer æºæ–‡ä»¶ï¼Œä¾‹å¦‚ï¼š
```bash
# ç¬¬2å¤©å­¦ä¹ æ–‡ä»¶
cat include/flashinfer/activation.cuh

# ç¬¬3å¤©å­¦ä¹ æ–‡ä»¶
cat include/flashinfer/math.cuh

# ç¬¬6å¤©å­¦ä¹ æ–‡ä»¶
cat include/flashinfer/norm.cuh
```

### 4. è¿è¡Œæµ‹è¯•éªŒè¯ç†è§£
```bash
# æµ‹è¯•æ¿€æ´»å‡½æ•°
pytest tests/utils/test_activation.py -v

# æµ‹è¯• norm
pytest tests/utils/test_norm.py -v
```

---

## ğŸ“– å­¦ä¹ è·¯å¾„æ€»ç»“

### ç¬¬ä¸€å‘¨ï¼šåŸºç¡€ä¸å†…å­˜ä¼˜åŒ–
```
Day 1: ç¯å¢ƒæ­å»º â†’ JIT ç¼–è¯‘ç†è§£
Day 2: å†…å­˜åˆå¹¶ â†’ å‘é‡åŒ–è®¿é—®
Day 3: PTX æ±‡ç¼– â†’ Warp shuffle
Day 4: å…±äº«å†…å­˜ â†’ Bank conflicts
Day 5: Warp ç¼–ç¨‹ â†’ Warp reduction
Day 6: Block reduction â†’ RMSNorm å®Œæ•´å®ç°
Day 7: å¼‚æ­¥æ‹·è´ â†’ cp.async æµæ°´çº¿
```

### ç¬¬äºŒå‘¨ï¼šé«˜çº§ç‰¹æ€§ä¸ç³»ç»Ÿé›†æˆ
```
Day 8:  CUB åº“ â†’ BlockScan/Reduce
Day 9:  Tensor Core â†’ MMA æŒ‡ä»¤
Day 10: GEMM ä¼˜åŒ– â†’ FP8/FP16
Day 11: Attention Decode â†’ Online Softmax
Day 12: Attention Prefill â†’ Multi-stage Pipeline
Day 13: JIT ç³»ç»Ÿ â†’ Jinja2 æ¨¡æ¿
Day 14: TVM-FFI â†’ PyTorch é›†æˆ
```

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆè¿™14å¤©è¯¾ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

âœ… ç†è§£ CUDA ç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ
âœ… ç¼–å†™é«˜æ€§èƒ½ GPU kernel
âœ… ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
âœ… ä½¿ç”¨ PTX æ±‡ç¼–è¿›è¡Œåº•å±‚ä¼˜åŒ–
âœ… å®ç°å¤æ‚çš„ reduction ç®—æ³•
âœ… ä½¿ç”¨ Tensor Core åŠ é€ŸçŸ©é˜µè¿ç®—
âœ… ç†è§£ Attention æœºåˆ¶çš„ GPU å®ç°
âœ… æŒæ¡ JIT ç¼–è¯‘ç³»ç»Ÿè®¾è®¡
âœ… å°† kernel é›†æˆåˆ° PyTorch ç­‰æ¡†æ¶

---

## ğŸ“š è¡¥å……èµ„æº

### FlashInfer ç›¸å…³
- **é¡¹ç›®æ–‡æ¡£**: [CLAUDE.md](./CLAUDE.md)
- **å®˜æ–¹æ–‡æ¡£**: https://docs.flashinfer.ai
- **æºç ä»“åº“**: https://github.com/flashinfer-ai/flashinfer

### CUDA å®˜æ–¹èµ„æº
- **ç¼–ç¨‹æŒ‡å—**: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- **PTX ISA**: https://docs.nvidia.com/cuda/parallel-thread-execution/
- **æœ€ä½³å®è·µ**: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/

### æ¨èå·¥å…·
- `nsight-compute` - Kernel æ€§èƒ½åˆ†æ
- `nsight-systems` - ç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- `cuda-gdb` - CUDA è°ƒè¯•å™¨
- `cuobjdump` - æŸ¥çœ‹ PTX/SASS ä»£ç 

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

1. **æ¯å¤© 2-3 å°æ—¶**ï¼šç†è®ºï¼ˆ30åˆ†é’Ÿï¼‰ + ä»£ç é˜…è¯»ï¼ˆ60åˆ†é’Ÿï¼‰ + å®è·µï¼ˆ60åˆ†é’Ÿï¼‰
2. **åŠ¨æ‰‹å®è·µ**ï¼šå¿…é¡»è¿è¡Œæµ‹è¯•ï¼Œä¿®æ”¹ä»£ç è§‚å¯Ÿæ•ˆæœ
3. **æ·±å…¥é˜…è¯»**ï¼šå®Œæ•´é˜…è¯»æ¯å¤©æŒ‡å®šçš„æºæ–‡ä»¶
4. **åšç¬”è®°**ï¼šè®°å½•å…³é”®æ¦‚å¿µå’Œä»£ç æ¨¡å¼
5. **æé—®æ€è€ƒ**ï¼šå®Œæˆæ¯å¤©çš„æ€è€ƒé¢˜
6. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ nsight-compute åˆ†æ kernel æ€§èƒ½

---

## âœ¨ ç‰¹è‰²

æœ¬è¯¾ç¨‹çš„ç‹¬ç‰¹ä¹‹å¤„ï¼š

1. **åŸºäºç”Ÿäº§çº§ä»£ç **ï¼šFlashInfer è¢« vLLMã€SGLangã€TensorRT-LLM ç­‰é¡¹ç›®ä½¿ç”¨
2. **å¾ªåºæ¸è¿›**ï¼šä»ç®€å•çš„æ¿€æ´»å‡½æ•°åˆ°å¤æ‚çš„ Attention kernel
3. **å®æˆ˜å¯¼å‘**ï¼šæ¯ä¸ªæ¦‚å¿µéƒ½æœ‰çœŸå®ä»£ç ç¤ºä¾‹
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ ä¸šç•Œæœ€ä½³å®è·µ
5. **å®Œæ•´é“¾è·¯**ï¼šä» CUDA kernel åˆ° Python API çš„å…¨æµç¨‹

---

## ğŸ“ å‰ç½®è¦æ±‚

- âœ… C/C++ ç¼–ç¨‹åŸºç¡€
- âœ… Python ç¼–ç¨‹åŸºç¡€
- âœ… åŸºæœ¬çš„çº¿æ€§ä»£æ•°çŸ¥è¯†
- âš ï¸ ä¸éœ€è¦ CUDA ç»éªŒï¼ˆä»é›¶å¼€å§‹ï¼‰

---

## ğŸ“§ åé¦ˆä¸æ”¹è¿›

å¦‚æœå‘ç°è¯¾ç¨‹ä¸­çš„é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿ï¼š
- æäº¤ Issue åˆ° FlashInfer ä»“åº“
- åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­åšç¬”è®°å¹¶åˆ†äº«

---

**ç¥å­¦ä¹ æ„‰å¿«ï¼é€šè¿‡å­¦ä¹  FlashInfer çš„é«˜æ€§èƒ½ kernelï¼Œä½ å°†æŒæ¡ GPU ç¼–ç¨‹çš„ç²¾é«“ï¼** ğŸš€
